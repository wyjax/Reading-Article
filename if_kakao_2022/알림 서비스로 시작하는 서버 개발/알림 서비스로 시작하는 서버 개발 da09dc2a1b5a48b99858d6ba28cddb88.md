# 알림 서비스로 시작하는 서버 개발

카카오뱅크가 만든 알림생성 이벤트

시스템

알림 발생 → 알림 서버에 알림 생성 요청 → 알림 디비에 저장

사용자

알림 조회 → 알림 서버에서 디비에서 조회

### 알림 서비스 장애

생성 서버와 조회 서버의 분리

생성 서버에 장애가 나더라도, 조회 서버는 조회 서비스를 제공할 수 있다. 관리 포인트가 존재하는게 단점이지만, 알림 서비스를 할 수 있게 된다.

### 신뢰성 없는 생성 서버

- 짧은 timeout
- 비동기

### 그럼 신뢰성은?

생성서버 내부에서 비동기로 알림을 생성

1. 연계서버 → 생성서버 알림 요청
2. 비동기하게 알림 생성
3. 응답

### 생성 서버가 다운되거나 재시작되면?

알림이 유실 될 수도 있다. 그래서 유실이되면 재처리 하도록 하는 방식이 처리되어야 하기 때문에, 이러한 부분에서 메시지 큐를 도입해서 줄여보기로 도전

![Untitled](%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%85%E1%85%B5%E1%86%B7%20%E1%84%89%E1%85%A5%E1%84%87%E1%85%B5%E1%84%89%E1%85%B3%E1%84%85%E1%85%A9%20%E1%84%89%E1%85%B5%E1%84%8C%E1%85%A1%E1%86%A8%E1%84%92%E1%85%A1%E1%84%82%E1%85%B3%E1%86%AB%20%E1%84%89%E1%85%A5%E1%84%87%E1%85%A5%20%E1%84%80%E1%85%A2%E1%84%87%E1%85%A1%E1%86%AF%20da09dc2a1b5a48b99858d6ba28cddb88/Untitled.png)

메시지큐를 도입하면서 알림을 처리하면서 컨슈머에서 장애가 나더라도 메시지큐가 ack를 수신하지 않아 메시지가 빠지지 않고 추후에 다시 서버가 일어났을 경우에 재처리가 가능하다. (관리 포인트가 줄어듬) 

### 완성된 아키텍처

![Untitled](%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%85%E1%85%B5%E1%86%B7%20%E1%84%89%E1%85%A5%E1%84%87%E1%85%B5%E1%84%89%E1%85%B3%E1%84%85%E1%85%A9%20%E1%84%89%E1%85%B5%E1%84%8C%E1%85%A1%E1%86%A8%E1%84%92%E1%85%A1%E1%84%82%E1%85%B3%E1%86%AB%20%E1%84%89%E1%85%A5%E1%84%87%E1%85%A5%20%E1%84%80%E1%85%A2%E1%84%87%E1%85%A1%E1%86%AF%20da09dc2a1b5a48b99858d6ba28cddb88/Untitled%201.png)

> 내 업무에 대한 이야기
나는 업무에서 메시지큐를 도입해서 처리하도록 한 경험이 있다. 서버는 이중화로 구성을 하였다. 하지만 연사님계서 발표하신 내용에서 queue가 다운되는 것을 대비해서 요청이 실패했을 경우에 생성 서버에서 DB에 저장하는 로직은 만들지 않았지만, 저러한 방식은 꽤나 유용해 보인다 왜냐하면 한 번 더 방어적인 서비스가 되기 떄문
> 

### 알림 조회

app → 알림 조회 → DB

### ‘고객에게 안정적이고 빠른 서비스 제공’

![Untitled](%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%85%E1%85%B5%E1%86%B7%20%E1%84%89%E1%85%A5%E1%84%87%E1%85%B5%E1%84%89%E1%85%B3%E1%84%85%E1%85%A9%20%E1%84%89%E1%85%B5%E1%84%8C%E1%85%A1%E1%86%A8%E1%84%92%E1%85%A1%E1%84%82%E1%85%B3%E1%86%AB%20%E1%84%89%E1%85%A5%E1%84%87%E1%85%A5%20%E1%84%80%E1%85%A2%E1%84%87%E1%85%A1%E1%86%AF%20da09dc2a1b5a48b99858d6ba28cddb88/Untitled%202.png)

하지만, 언제든 추가적인 요구사항이 들어올 수 밖에 없고 대비를 해야한다. 

### 요구사항

기존 사용자는 알림을 90일 동안만 보여준다.

- 고객센터 : 최대 12개월 전 알림까지 조회하고 싶다.
- 운영팀 : 대용량의 알림 텟플릿을 엑셀로 다운로드 / 업로드하고 싶다.

위의 요구사항을 해결하기 위해서 카뱅팀에서는 고객 서비스와 운영(내부) 서비스를 분리해서 처리하기로 했다. 하지만 서비스만 분리할 게 아니라 archive DB를 추가해서 분리.

![Untitled](%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%85%E1%85%B5%E1%86%B7%20%E1%84%89%E1%85%A5%E1%84%87%E1%85%B5%E1%84%89%E1%85%B3%E1%84%85%E1%85%A9%20%E1%84%89%E1%85%B5%E1%84%8C%E1%85%A1%E1%86%A8%E1%84%92%E1%85%A1%E1%84%82%E1%85%B3%E1%86%AB%20%E1%84%89%E1%85%A5%E1%84%87%E1%85%A5%20%E1%84%80%E1%85%A2%E1%84%87%E1%85%A1%E1%86%AF%20da09dc2a1b5a48b99858d6ba28cddb88/Untitled%203.png)

서비스 디비는 crud가 빈번하게 일어나서 성능이 좋아야 하고, 아카이브 디비에는 많은 데이터가 저장되지만 빠른 서비스를 제공하지는 않아서 된다.

### 알림 파기

서비스 디비에서는 3개월 동안의 알림만 해주면 되기 때문에 디비를 없애줘야 한다. 

그렇다면 DELETE? DELETE FROM TABLE where (3개월이전), 이렇게 사용하면 한 순간에 많은 데이터를 삭제하면 건건히 락이 걸리거나 트랜잭션 io가 발생하거나 추가적인 작업으로 많은 부하가 걸려 서비스에 악영향이 있을 수 있다.

그래서 partitioning이다. 물리적으로는 같은 테이블이지만 논리적으로는 다른 테이블이다. 

![Untitled](%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%85%E1%85%B5%E1%86%B7%20%E1%84%89%E1%85%A5%E1%84%87%E1%85%B5%E1%84%89%E1%85%B3%E1%84%85%E1%85%A9%20%E1%84%89%E1%85%B5%E1%84%8C%E1%85%A1%E1%86%A8%E1%84%92%E1%85%A1%E1%84%82%E1%85%B3%E1%86%AB%20%E1%84%89%E1%85%A5%E1%84%87%E1%85%A5%20%E1%84%80%E1%85%A2%E1%84%87%E1%85%A1%E1%86%AF%20da09dc2a1b5a48b99858d6ba28cddb88/Untitled%204.png)

파티셔닝이 되면 9월의 데이터만 지우면 아무런 무리없이 될 수 있다. 하지만 서비스가 복잡해지고 알림 수가 늘어나면 대비가 필요하다.

### application scale-out

![Untitled](%E1%84%8B%E1%85%A1%E1%86%AF%E1%84%85%E1%85%B5%E1%86%B7%20%E1%84%89%E1%85%A5%E1%84%87%E1%85%B5%E1%84%89%E1%85%B3%E1%84%85%E1%85%A9%20%E1%84%89%E1%85%B5%E1%84%8C%E1%85%A1%E1%86%A8%E1%84%92%E1%85%A1%E1%84%82%E1%85%B3%E1%86%AB%20%E1%84%89%E1%85%A5%E1%84%87%E1%85%A5%20%E1%84%80%E1%85%A2%E1%84%87%E1%85%A1%E1%86%AF%20da09dc2a1b5a48b99858d6ba28cddb88/Untitled%205.png)

**‘애플리케이션의 cpu 사용률이 50%를 넘어가면 서버 증설을 고려’**

하지만 애플리케이션만 늘어나서 해결될 게 아니라, 샤딩 기법을 사요했다.  

### 샤딩

같은 테이블 구조를 가진 데이터를 분산해서 저장하는 기법. (하지만 앱이 복잡해질 수 있다) a-z까지 3개의 디비로 나누어서 작업을 하고 있었다가 데이터가 많아져서 3개의 디비를 4개로 늘였을 경우! 데이터를 나누는 작업이 또 필요해지게 된다. 

### Reference

[https://www.youtube.com/watch?v=CmTO68I2HSc](https://www.youtube.com/watch?v=CmTO68I2HSc)